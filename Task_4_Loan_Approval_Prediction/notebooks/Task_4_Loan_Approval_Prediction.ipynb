{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df04e79",
   "metadata": {},
   "source": [
    "# Task 4: Loan Approval Prediction\n",
    "## Binary Classification with Imbalanced Data Handling\n",
    "\n",
    "**Objective:** Build a robust model to predict loan approval status\n",
    "\n",
    "**Key Features:**\n",
    "- Binary classification using Logistic Regression and Decision Tree\n",
    "- Handle class imbalance with SMOTE\n",
    "- Evaluate performance on imbalanced data\n",
    "- Focus on precision, recall, and F1-score\n",
    "- Model comparison and selection\n",
    "\n",
    "**Dataset:** Loan-Approval-Prediction-Dataset (200+ loan applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137de0bf",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31073eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print('‚úÖ All libraries imported successfully!')\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c4c25",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load from online source first, then local fallback\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/siddharth-daga/Loan-Approval-Dataset/master/loan_data.csv',\n",
    "    'https://kaggle.com/api/v1/datasets/download/altruistdream/loan-approval-prediction-dataset'\n",
    "]\n",
    "\n",
    "local_files = [\n",
    "    'loan_data.csv',\n",
    "    'loan_approval.csv',\n",
    "    'Loan_Approval_Dataset.csv',\n",
    "    '../Loan_Approval_Dataset.csv'\n",
    "]\n",
    "\n",
    "df = None\n",
    "file_path = None\n",
    "\n",
    "# Try online sources\n",
    "for url in urls:\n",
    "    try:\n",
    "        print(f\"Attempting to load from: {url}\")\n",
    "        df = pd.read_csv(url)\n",
    "        file_path = url\n",
    "        print(f\"‚úÖ Successfully loaded from online source!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed: {str(e)[:50]}...\")\n",
    "        continue\n",
    "\n",
    "# Try local files if online failed\n",
    "if df is None:\n",
    "    for file in local_files:\n",
    "        if os.path.exists(file):\n",
    "            try:\n",
    "                print(f\"\\nAttempting to load from: {file}\")\n",
    "                df = pd.read_csv(file)\n",
    "                file_path = file\n",
    "                print(f\"‚úÖ Successfully loaded from local file!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed: {str(e)[:50]}...\")\n",
    "                continue\n",
    "\n",
    "if df is None:\n",
    "    # Create a sample dataset if no file found\n",
    "    print(\"\\n‚ö†Ô∏è No dataset found. Creating sample loan data...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 300\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Loan_ID': range(1, n_samples + 1),\n",
    "        'Age': np.random.randint(22, 70, n_samples),\n",
    "        'Income': np.random.randint(20000, 150000, n_samples),\n",
    "        'Credit_Score': np.random.randint(300, 850, n_samples),\n",
    "        'Employment_Years': np.random.randint(0, 40, n_samples),\n",
    "        'Loan_Amount': np.random.randint(5000, 500000, n_samples),\n",
    "        'Gender': np.random.choice(['M', 'F'], n_samples),\n",
    "        'Married': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'Dependents': np.random.randint(0, 4, n_samples),\n",
    "        'Education': np.random.choice(['Graduate', 'Not Graduate'], n_samples),\n",
    "        'Self_Employed': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'Approval': np.random.choice(['Approved', 'Rejected'], n_samples, p=[0.7, 0.3])\n",
    "    })\n",
    "    file_path = 'sample_loan_data'\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset: {file_path}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nColumn Information:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fac68",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing & Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b90a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Handle missing values\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    else:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Missing values handled!\")\n",
    "\n",
    "# Identify target and features\n",
    "target_cols = ['Approval', 'Loan_Status', 'Status', 'Approved']\n",
    "target_col = None\n",
    "for col in target_cols:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    print(\"‚ö†Ô∏è Target column not found. Using last column as target.\")\n",
    "    target_col = df.columns[-1]\n",
    "\n",
    "print(f\"\\nTarget Column: {target_col}\")\n",
    "print(f\"Target Distribution:\")\n",
    "print(df[target_col].value_counts())\n",
    "print(f\"\\nTarget Distribution (%):\\n{df[target_col].value_counts(normalize=True) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879181c1",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4526c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Store column names for later\n",
    "feature_columns = X.columns.tolist()\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical features ({len(numerical_cols)}): {numerical_cols}\")\n",
    "\n",
    "# Remove ID columns if present\n",
    "id_cols = [col for col in numerical_cols if 'ID' in col or 'id' in col]\n",
    "if id_cols:\n",
    "    print(f\"\\nRemoving ID columns: {id_cols}\")\n",
    "    X = X.drop(columns=id_cols)\n",
    "    numerical_cols = [col for col in numerical_cols if col not in id_cols]\n",
    "\n",
    "# Encode target variable\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "print(f\"\\nTarget classes: {le_target.classes_}\")\n",
    "print(f\"Encoded: {np.unique(y_encoded)}\")\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"Encoded {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Feature engineering complete!\")\n",
    "print(f\"Final feature set shape: {X.shape}\")\n",
    "print(f\"Target shape: {y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7310518",
   "metadata": {},
   "source": [
    "## 5. Class Imbalance Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2912860",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä CLASS IMBALANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate class distribution\n",
    "unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "class_distribution = dict(zip(le_target.classes_, counts))\n",
    "\n",
    "print(f\"\\nClass Distribution:\")\n",
    "for cls, count in class_distribution.items():\n",
    "    pct = (count / len(y_encoded)) * 100\n",
    "    print(f\"  {cls}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "imbalance_ratio = counts.max() / counts.min()\n",
    "print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "axes[0].bar(le_target.classes_, counts, color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Loan Approval Distribution (Before SMOTE)', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(counts, labels=le_target.classes_, autopct='%1.1f%%', \n",
    "            colors=['#FF6B6B', '#4ECDC4'], startangle=90)\n",
    "axes[1].set_title('Class Distribution %', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/01_class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Saved: outputs/01_class_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ca331",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split & Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÄ TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(le_target.classes_, counts_train):\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    print(f\"  {cls}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n‚úÖ Data scaled using StandardScaler\")\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e680bf",
   "metadata": {},
   "source": [
    "## 7. Apply SMOTE to Handle Class Imbalance (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öñÔ∏è APPLYING SMOTE (Synthetic Minority Over-sampling Technique)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nOriginal training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"SMOTE training set shape: {X_train_smote.shape}\")\n",
    "\n",
    "print(f\"\\nOriginal class distribution (training):\")\n",
    "unique_orig, counts_orig = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(le_target.classes_, counts_orig):\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    print(f\"  {cls}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nAfter SMOTE class distribution (training):\")\n",
    "unique_smote, counts_smote = np.unique(y_train_smote, return_counts=True)\n",
    "for cls, count in zip(le_target.classes_, counts_smote):\n",
    "    pct = (count / len(y_train_smote)) * 100\n",
    "    print(f\"  {cls}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "# Visualize SMOTE effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(le_target.classes_, counts_orig, color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Class Distribution Before SMOTE', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(le_target.classes_, counts_smote, color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Class Distribution After SMOTE', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/02_smote_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Saved: outputs/02_smote_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecf7ad",
   "metadata": {},
   "source": [
    "## 8. Model Training: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408ec1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nü§ñ TRAINING LOGISTIC REGRESSION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train Logistic Regression on SMOTE data\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "lr_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_lr = cross_val_score(lr_model, X_train_smote, y_train_smote, cv=5, scoring='f1')\n",
    "\n",
    "print(f\"\\n‚úÖ Logistic Regression trained successfully!\")\n",
    "print(f\"Cross-validation F1 scores: {cv_scores_lr}\")\n",
    "print(f\"Mean CV F1-Score: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2865a357",
   "metadata": {},
   "source": [
    "## 9. Model Training: Decision Tree (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c0b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüå≥ TRAINING DECISION TREE MODEL (BONUS)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train Decision Tree on SMOTE data\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=10, \n",
    "    random_state=42, \n",
    "    class_weight='balanced',\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2\n",
    ")\n",
    "dt_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores_dt = cross_val_score(dt_model, X_train_smote, y_train_smote, cv=5, scoring='f1')\n",
    "\n",
    "print(f\"\\n‚úÖ Decision Tree trained successfully!\")\n",
    "print(f\"Cross-validation F1 scores: {cv_scores_dt}\")\n",
    "print(f\"Mean CV F1-Score: {cv_scores_dt.mean():.4f} (+/- {cv_scores_dt.std():.4f})\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance_dt.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184d642",
   "metadata": {},
   "source": [
    "## 10. Comprehensive Model Evaluation (Focus on Imbalanced Data Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä MODEL EVALUATION ON IMBALANCED DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Logistic Regression Evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_lr):.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=le_target.classes_))\n",
    "\n",
    "# Decision Tree Evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DECISION TREE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba_dt):.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Decision Tree):\")\n",
    "print(classification_report(y_test, y_pred_dt, target_names=le_target.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b41fa4",
   "metadata": {},
   "source": [
    "## 11. Confusion Matrices & ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Confusion Matrix - Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0], \n",
    "            xticklabels=le_target.classes_, yticklabels=le_target.classes_)\n",
    "axes[0, 0].set_title('Confusion Matrix - Logistic Regression', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('True Label')\n",
    "axes[0, 0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Confusion Matrix - Decision Tree\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Greens', ax=axes[0, 1],\n",
    "            xticklabels=le_target.classes_, yticklabels=le_target.classes_)\n",
    "axes[0, 1].set_title('Confusion Matrix - Decision Tree', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('True Label')\n",
    "axes[0, 1].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curves\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)\n",
    "\n",
    "auc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "auc_dt = roc_auc_score(y_test, y_pred_proba_dt)\n",
    "\n",
    "axes[1, 0].plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC={auc_lr:.3f})', linewidth=2)\n",
    "axes[1, 0].plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC={auc_dt:.3f})', linewidth=2)\n",
    "axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[1, 0].set_xlabel('False Positive Rate')\n",
    "axes[1, 0].set_ylabel('True Positive Rate')\n",
    "axes[1, 0].set_title('ROC Curves Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_pred_proba_lr)\n",
    "precision_dt, recall_dt, _ = precision_recall_curve(y_test, y_pred_proba_dt)\n",
    "\n",
    "pr_auc_lr = auc(recall_lr, precision_lr)\n",
    "pr_auc_dt = auc(recall_dt, precision_dt)\n",
    "\n",
    "axes[1, 1].plot(recall_lr, precision_lr, label=f'Logistic Regression (AUC={pr_auc_lr:.3f})', linewidth=2)\n",
    "axes[1, 1].plot(recall_dt, precision_dt, label=f'Decision Tree (AUC={pr_auc_dt:.3f})', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Recall')\n",
    "axes[1, 1].set_ylabel('Precision')\n",
    "axes[1, 1].set_title('Precision-Recall Curves', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/03_model_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Saved: outputs/03_model_evaluation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408e9e7",
   "metadata": {},
   "source": [
    "## 12. Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèÜ MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_dt)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_pred_lr),\n",
    "        precision_score(y_test, y_pred_dt)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_pred_lr),\n",
    "        recall_score(y_test, y_pred_dt)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_lr),\n",
    "        f1_score(y_test, y_pred_dt)\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_test, y_pred_proba_lr),\n",
    "        roc_auc_score(y_test, y_pred_proba_dt)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model based on F1-score\n",
    "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
    "best_f1 = comparison_df.loc[best_model_idx, 'F1-Score']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} (F1-Score: {best_f1:.4f})\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    values = comparison_df[metric].values\n",
    "    axes[idx].bar(comparison_df['Model'], values, color=colors)\n",
    "    axes[idx].set_ylabel(metric, fontsize=12)\n",
    "    axes[idx].set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(values):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/04_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Saved: outputs/04_model_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed7af9",
   "metadata": {},
   "source": [
    "## 13. Feature Importance Analysis (BONUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae50552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Decision Tree Feature Importance\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nDecision Tree - Top 10 Features:\")\n",
    "print(feature_importance_dt.head(10))\n",
    "\n",
    "# Logistic Regression Coefficients\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': np.abs(lr_model.coef_[0])\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nLogistic Regression - Top 10 Features (by coefficient magnitude):\")\n",
    "print(feature_importance_lr.head(10))\n",
    "\n",
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Decision Tree\n",
    "top_dt = feature_importance_dt.head(10)\n",
    "axes[0].barh(top_dt['Feature'], top_dt['Importance'], color='#4ECDC4')\n",
    "axes[0].set_xlabel('Importance', fontsize=12)\n",
    "axes[0].set_title('Decision Tree - Top 10 Important Features', fontsize=13, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Logistic Regression\n",
    "top_lr = feature_importance_lr.head(10)\n",
    "axes[1].barh(top_lr['Feature'], top_lr['Coefficient'], color='#FF6B6B')\n",
    "axes[1].set_xlabel('Coefficient Magnitude', fontsize=12)\n",
    "axes[1].set_title('Logistic Regression - Top 10 Important Features', fontsize=13, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/05_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úÖ Saved: outputs/05_feature_importance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74507af",
   "metadata": {},
   "source": [
    "## 14. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ee277",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíæ SAVING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "joblib.dump(lr_model, 'model/logistic_regression_model.pkl')\n",
    "joblib.dump(dt_model, 'model/decision_tree_model.pkl')\n",
    "joblib.dump(scaler, 'model/scaler.pkl')\n",
    "joblib.dump(label_encoders, 'model/label_encoders.pkl')\n",
    "joblib.dump(le_target, 'model/target_encoder.pkl')\n",
    "\n",
    "print(\"\\n‚úÖ Models saved:\")\n",
    "print(\"   - model/logistic_regression_model.pkl\")\n",
    "print(\"   - model/decision_tree_model.pkl\")\n",
    "print(\"   - model/scaler.pkl\")\n",
    "print(\"   - model/label_encoders.pkl\")\n",
    "print(\"   - model/target_encoder.pkl\")\n",
    "\n",
    "# Save feature columns for later use\n",
    "joblib.dump(X.columns.tolist(), 'model/feature_columns.pkl')\n",
    "print(\"   - model/feature_columns.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c84790",
   "metadata": {},
   "source": [
    "## 15. Test Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîÆ TESTING PREDICTIONS ON NEW DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create sample new customers for prediction\n",
    "new_customers = pd.DataFrame({\n",
    "    'Age': [35, 45, 28],\n",
    "    'Income': [50000, 120000, 35000],\n",
    "    'Credit_Score': [720, 800, 650],\n",
    "    'Employment_Years': [5, 15, 2],\n",
    "    'Loan_Amount': [100000, 300000, 50000],\n",
    "    'Gender': ['M', 'F', 'M'],\n",
    "    'Married': ['Yes', 'Yes', 'No'],\n",
    "    'Dependents': [1, 2, 0],\n",
    "    'Education': ['Graduate', 'Graduate', 'Not Graduate'],\n",
    "    'Self_Employed': ['No', 'No', 'Yes']\n",
    "})\n",
    "\n",
    "# Make predictions if all required columns exist\n",
    "try:\n",
    "    # Encode categorical features\n",
    "    new_customers_encoded = new_customers.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in new_customers_encoded.columns:\n",
    "            new_customers_encoded[col] = label_encoders[col].transform(new_customers_encoded[col].astype(str))\n",
    "    \n",
    "    # Scale features\n",
    "    new_customers_scaled = scaler.transform(new_customers_encoded[X.columns])\n",
    "    \n",
    "    # Predictions\n",
    "    pred_lr = lr_model.predict(new_customers_scaled)\n",
    "    pred_proba_lr = lr_model.predict_proba(new_customers_scaled)\n",
    "    \n",
    "    pred_dt = dt_model.predict(new_customers_scaled)\n",
    "    pred_proba_dt = dt_model.predict_proba(new_customers_scaled)\n",
    "    \n",
    "    print(\"\\nSample New Customers:\")\n",
    "    print(new_customers.to_string())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PREDICTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(len(new_customers)):\n",
    "        print(f\"\\nCustomer {i+1}:\")\n",
    "        print(f\"  Age: {new_customers.iloc[i]['Age']}, Income: ${new_customers.iloc[i]['Income']:,}\")\n",
    "        print(f\"  Credit Score: {new_customers.iloc[i]['Credit_Score']}, Employment: {new_customers.iloc[i]['Employment_Years']} years\")\n",
    "        \n",
    "        print(f\"  \\n  Logistic Regression:\")\n",
    "        print(f\"    Prediction: {le_target.classes_[pred_lr[i]]}\")\n",
    "        print(f\"    Confidence: {pred_proba_lr[i].max():.2%}\")\n",
    "        \n",
    "        print(f\"  \\n  Decision Tree:\")\n",
    "        print(f\"    Prediction: {le_target.classes_[pred_dt[i]]}\")\n",
    "        print(f\"    Confidence: {pred_proba_dt[i].max():.2%}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not create sample predictions: {e}\")\n",
    "    print(\"This is expected if categorical columns don't match the training data exactly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df3ff2",
   "metadata": {},
   "source": [
    "## 16. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d32f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã TASK 4 EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "LOAN APPROVAL PREDICTION - BINARY CLASSIFICATION\n",
    "{'='*60}\n",
    "\n",
    "üìä DATASET INFORMATION:\n",
    "   Total Samples: {len(df)}\n",
    "   Features: {len(feature_columns)}\n",
    "   Target: {target_col}\n",
    "   Classes: {', '.join(le_target.classes_)}\n",
    "   Class Imbalance Ratio: {imbalance_ratio:.2f}:1\n",
    "\n",
    "üîÑ DATA PREPROCESSING:\n",
    "   Missing Values: Handled with mode/median imputation\n",
    "   Categorical Features: {len(categorical_cols)} features encoded\n",
    "   Feature Scaling: StandardScaler applied\n",
    "   SMOTE Applied: YES (balanced training data)\n",
    "\n",
    "ü§ñ MODELS TRAINED:\n",
    "   1. Logistic Regression\n",
    "      - Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\n",
    "      - Precision: {precision_score(y_test, y_pred_lr):.4f}\n",
    "      - Recall: {recall_score(y_test, y_pred_lr):.4f}\n",
    "      - F1-Score: {f1_score(y_test, y_pred_lr):.4f}\n",
    "      - ROC-AUC: {roc_auc_score(y_test, y_pred_proba_lr):.4f}\n",
    "\n",
    "   2. Decision Tree (BONUS)\n",
    "      - Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\n",
    "      - Precision: {precision_score(y_test, y_pred_dt):.4f}\n",
    "      - Recall: {recall_score(y_test, y_pred_dt):.4f}\n",
    "      - F1-Score: {f1_score(y_test, y_pred_dt):.4f}\n",
    "      - ROC-AUC: {roc_auc_score(y_test, y_pred_proba_dt):.4f}\n",
    "\n",
    "üèÜ BEST MODEL: {best_model_name}\n",
    "   F1-Score: {best_f1:.4f}\n",
    "\n",
    "üìÅ FILES GENERATED:\n",
    "   Models:\n",
    "      - model/logistic_regression_model.pkl\n",
    "      - model/decision_tree_model.pkl\n",
    "      - model/scaler.pkl\n",
    "      - model/label_encoders.pkl\n",
    "      - model/target_encoder.pkl\n",
    "      - model/feature_columns.pkl\n",
    "\n",
    "   Visualizations:\n",
    "      - outputs/01_class_distribution.png\n",
    "      - outputs/02_smote_comparison.png\n",
    "      - outputs/03_model_evaluation.png\n",
    "      - outputs/04_model_comparison.png\n",
    "      - outputs/05_feature_importance.png\n",
    "\n",
    "‚úÖ TASK COMPLETION: 100%\n",
    "   ‚úì EDA completed\n",
    "   ‚úì Data preprocessing done\n",
    "   ‚úì Class imbalance handled with SMOTE\n",
    "   ‚úì Two models trained and compared\n",
    "   ‚úì Comprehensive evaluation on imbalanced data\n",
    "   ‚úì Feature importance analyzed\n",
    "   ‚úì Models saved for deployment\n",
    "\n",
    "üöÄ NEXT STEP: Build Streamlit app for interactive predictions\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary to file\n",
    "with open('outputs/summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n‚úÖ Summary saved to: outputs/summary.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
